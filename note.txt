Document (PDF/Word) with all sections above
README file with run instructions
Source code (GitHub/zip)
Dataset file
Results summary and evidence (screenshots/logs/video)

# Install Java
sudo apt-get update
sudo apt-get install -y default-jdk
java -version

# Create a Dedicated Hadoop User
## Create a new user (e.g., hadoop) to isolate Hadoop from your main system
sudo adduser hadoop
sudo usermod -aG sudo hadoop
## Switch to the new user
sudo su - hadoop

# Configure SSH for Hadoop User
## Install SSH server and client
sudo apt-get install openssh-server openssh-client -y
## Generate SSH keys (press Enter at prompts)
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
## Test passwordless SSH
ssh localhost

# Download and Install Hadoop
## Download Hadoop (choose the latest stable version)
wget https://downloads.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz
tar -xzf hadoop-3.4.0.tar.gz
## Move Hadoop to /usr/local for standardization
sudo mv hadoop-3.4.0 /usr/local/hadoop
sudo mkdir /usr/local/hadoop/logs
sudo chown -R hadoop:hadoop /usr/local/hadoop

# Set Hadoop Environment Variables
## Edit your .bashrc file
nano ~/.bashrc
## Apply the changes
source ~/.bashrc

# Configure Hadoop (Basic)
core-site.xml
hdfs-site.xml
mapred-site.xml
yarn-site.xml
hadoop-env.sh (set JAVA_HOME)

# Format the NameNode
hdfs namenode -format

# Start Hadoop Services
## Start the Hadoop daemons
start-dfs.sh
start-yarn.sh
## Check running Java processes
jps
## How to Fix Incompatible clusterIDs (NameNode vs DataNode)
stop-all.sh
rm -rf /home/hadoop/hadoopdata/hdfs/datanode/*
start-all.sh
jps

# Verify Installation
## Access the Hadoop web UI
NameNode: http://localhost:9870
ResourceManager: http://localhost:8088
## Test HDFS commands
hdfs dfs -mkdir /test
hdfs dfs -ls /

# Re-upload the Input File to HDFS
hdfs dfs -mkdir -p /user/hadoop/spotify
hdfs dfs -ls /user/hadoop/spotify
hdfs dfs -put /home/malith/Desktop/assignment_1/spotify_dataset.csv /user/hadoop/spotify/
hdfs dfs -ls /user/hadoop/spotify

# Restart Hadoop Services
stop-all.sh
start-all.sh

# Copy python files
cp /home/malith/Desktop/assignment_1/mapper.py /tmp/
cp /home/malith/Desktop/assignment_1/reducer.py /tmp/
chmod +x /tmp/mapper.py /tmp/reducer.py

# Before running your Hadoop job, remove the output directory from HDFS
hdfs dfs -rm -r /user/hadoop/spotify/output

# Run the MapReduce Job with Hadoop Streaming
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -input /user/hadoop/spotify/spotify_dataset.csv \
  -output /user/hadoop/spotify/output \
  -mapper /tmp/mapper.py \
  -reducer /tmp/reducer.py

# To view the output directly in the terminal
hdfs dfs -cat /user/hadoop/spotify/output/part-*

# To view just the first 20 lines
hdfs dfs -cat /user/hadoop/spotify/output/part-* | head -n 20

# To copy the output from HDFS to your local file system
mkdir -p /home/hadoop/spotify_output
hdfs dfs -get /user/hadoop/spotify/output/part-* /home/hadoop/spotify_output/

# To merge all output files into a single local file
hdfs dfs -getmerge /user/hadoop/spotify/output/ /home/hadoop/spotify_output.txt
